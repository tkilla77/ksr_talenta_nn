{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loose-queensland",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in Python with Keras\n",
    "\n",
    "\n",
    "This is adapted from https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "\n",
    "Notable changes:\n",
    " * switched to MNIST numbers dataset\n",
    " * only training two epochs\n",
    " * when the doc talks about \"dense\" layers, it means the fully connected layers we created in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-province",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network: Introduction\n",
    "\n",
    "A specific kind of a deep neural network is the convolutional network, which is commonly referred to as CNN or ConvNet. It's a deep, feed-forward artificial neural network. Remember that feed-forward neural networks are also called multi-layer perceptrons(MLPs), which are the quintessential deep learning models. The models are called \"feed-forward\" because information flï¿½ows right through the model. There are no feedback connections in which outputs of the model are fed back into itself.\n",
    "\n",
    "CNNs specifically are inspired by the biological visual cortex. The cortex has small regions of cells that are sensitive to the specific areas of the visual field. This idea was expanded by a captivating experiment done by Hubel and Wiesel in 1962 (if you want to know more, here's a video). In this experiment, the researchers showed that some individual neurons in the brain activated or fired only in the presence of edges of a particular orientation like vertical or horizontal edges. For example, some neurons fired when exposed to vertical sides and some when shown a horizontal edge. Hubel and Wiesel found that all of these neurons were well ordered in a columnar fashion and that together they were able to produce visual perception. This idea of specialized components inside of a system having specific tasks is one that machines use as well and one that you can also find back in CNNs.\n",
    "\n",
    "Convolutional neural networks have been one of the most influential innovations in the field of computer vision. They have performed a lot better than traditional computer vision and have produced state-of-the-art results. These neural networks have proven to be successful in many different real-life case studies and applications, like:\n",
    "\n",
    " * Image classification, object detection, segmentation, face recognition;\n",
    " * Self driving cars that leverage CNN based vision systems;\n",
    " * Classification of crystal structure using a convolutional neural network;\n",
    "\n",
    "And many more, of course!\n",
    "To understand this success, you'll have to go back to 2012, the year in which Alex Krizhevsky used convolutional neural networks to win that year's ImageNet Competition, reducing the classification error from 26% to 15%.\n",
    "\n",
    "Note that ImageNet Large Scale Visual Recognition Challenge (ILSVRC) began in the year 2010 is an annual competition where research teams assess their algorithms on the given data set and compete to achieve higher accuracy on several visual recognition tasks.\n",
    "\n",
    "This was the time when neural networks regained prominence after quite some time. This is often called the \"third wave of neural networks\". The other two waves were in the 1940s until the 1960s and in the 1970s to 1980s.\n",
    "\n",
    "Alright, you know that you'll be working with feed-forward networks that are inspired by the biological visual cortex, but what does that actually mean?\n",
    "\n",
    "Take a look at the picture below:\n",
    "\n",
    "![Figure: Convolutional Neural Network from Wikimedia](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
    "\n",
    "The image shows you that you feed an image as an input to the network, which goes through multiple convolutions, subsampling, a fully connected layer and finally outputs something.\n",
    "\n",
    "But what are all these concepts?\n",
    "\n",
    " - The convolution layer computes the output of neurons that are connected to local regions or receptive fields in the input, each computing a dot product between their weights and a small receptive field to which they are connected to in the input volume. Each computation leads to extraction of a feature map from the input image. In other words, imagine you have an image represented as a 5x5 matrix of values, and you take a 3x3 matrix and slide that 3x3 window or kernel around the image. At each position of that matrix, you multiply the values of your 3x3 window by the values in the image that are currently being covered by the window. As a result, you'll get a single number that represents all the values in that window of the images. You use this layer to filtering: as the window moves over the image, you check for patterns in that section of the image. This works because of filters, which are multiplied by the values outputted by the convolution.\n",
    " - The objective of subsampling is to get an input representation by reducing its dimensions, which helps in reducing overfitting. One of the techniques of subsampling is max pooling. With this technique, you select the highest pixel value from a region depending on its size. In other words, max pooling takes the largest value from the window of the image currently covered by the kernel. For example, you can have a max-pooling layer of size 2 x 2 will select the maximum pixel intensity value from 2 x 2 region. You're right to think that the pooling layer then works a lot like the convolution layer! You also take a kernel or a window and move it over the image; The only difference is the function that is applied to the kernel and the image window isn't linear.\n",
    "\n",
    "![Figure: Max-Pooling from Wikipedia](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n",
    "\n",
    " - The objective of the fully connected layer is to flatten the high-level features that are learned by convolutional layers and combining all the features. It passes the flattened output to the output layer where you use a softmax classifier or a sigmoid to predict the input class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mysterious-turning",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (3.17.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (0.36.2)\n",
      "Requirement already satisfied: scipy==1.4.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (0.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.21.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.38.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.6.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorflow==2.2.0) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.32.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.26.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.0)\n",
      "Requirement already satisfied: keras in /srv/conda/envs/notebook/lib/python3.7/site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.7/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from keras) (1.21.0)\n",
      "Requirement already satisfied: h5py in /srv/conda/envs/notebook/lib/python3.7/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.7/site-packages (3.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (8.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.16 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (1.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[K     |ââââââââââââââââââââââââââââââââ| 22.3 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.0)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |ââââââââââââââââââââââââââââââââ| 303 kB 57.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=76223e057f8fc13f5977e3c160fb430b0ccd32e6c2e134f511c96543620c702a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.1 scikit-learn-0.24.2 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2.0\n",
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-willow",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Keras comes with a library called datasets, which you can use to load datasets out of the box: you download the data from the server and speeds up the process since you no longer have to download the data to your computer. The train and test images along with the labels are loaded and stored in variables train_X, train_Y, test_X, test_Y, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "devoted-veteran",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_X,train_Y), (test_X,test_Y) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-crawford",
   "metadata": {},
   "source": [
    "Great! That was pretty simple, wasn't it?\n",
    "\n",
    "You have probably done this a million times by now, but it's always an essential step to get started. Now you're completely set to start analyzing, processing and modeling your data!\n",
    "\n",
    "\n",
    "## Analyze the Data\n",
    "Let's now analyze how images in the dataset look like. Even though you know the dimension of the images by now, it's still worth the effort to analyze it programmatically: you might have to rescale the image pixels and resize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modular-shepherd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (60000, 28, 28) (60000,)\n",
      "Testing data shape :  (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('Training data shape : ', train_X.shape, train_Y.shape)\n",
    "\n",
    "print('Testing data shape : ', test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-magazine",
   "metadata": {},
   "source": [
    "From the above output, you can see that the training data has a shape of 60000 x 28 x 28 since there are 60,000 training samples each of 28 x 28 dimension. Similarly, the test data has a shape of 10000 x 28 x 28 since there are 10,000 testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "objective-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  10\n",
      "Output classes :  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(train_Y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-myrtle",
   "metadata": {},
   "source": [
    "There's also a total of ten output classes that range from 0 to 9.\n",
    "\n",
    "Also, don't forget to take a look at what the images in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bright-concern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ground Truth : 7')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(train_Y[0]))\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(test_Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-father",
   "metadata": {},
   "source": [
    "The output of above two plots looks like 5 and a 7.\n",
    "\n",
    "\n",
    "## Data Preprocessing\n",
    "As you could see in the above plot, the images are grayscale images have pixel values that range from 0 to 255. Also, these images have a dimension of 28 x 28. As a result, you'll need to preprocess the data before you feed it into the model.\n",
    "\n",
    "As a first step, convert each 28 x 28 image of the train and test set into a matrix of size 28 x 28 x 1 which is fed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "parallel-feeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_X.reshape(-1, 28,28, 1)\n",
    "test_X = test_X.reshape(-1, 28,28, 1)\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-pharmacology",
   "metadata": {},
   "source": [
    "The data right now is in an int8 format, so before you feed it into the network you need to convert its type to float32, and you also have to rescale the pixel values in range 0 - 1 inclusive. So let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "disturbed-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-concept",
   "metadata": {},
   "source": [
    "Now you need to convert the class labels into a one-hot encoding vector.\n",
    "In one-hot encoding, you convert the categorical data into a vector of numbers. The reason why you convert the categorical data in one hot encoding is that machine learning algorithms cannot work with categorical data directly. You generate one boolean column for each category or class. Only one of these columns could take on the value 1 for each sample. Hence, the term one-hot encoding.\n",
    "\n",
    "For your problem statement, the one hot encoding will be a row vector, and for each image, it will have a dimension of 1 x 10. The important thing to note here is that the vector consists of all zeros except for the class that it represents, and for that, it is 1. For example, the ankle boot image that you plotted above has a label of 9, so for all the ankle boot images, the one hot encoding vector would be [0 0 0 0 0 0 0 0 1 0].\n",
    "\n",
    "So let's convert the training and testing labels into one-hot encoding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "organic-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 5\n",
      "After conversion to one-hot: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-fleet",
   "metadata": {},
   "source": [
    "That's pretty clear, right? Note that you can also print the train_Y_one_hot, which will display a matrix of size 60000 x 10 in which each row depicts one-hot encoding of an image.\n",
    "\n",
    "This last step is a crucial one. In machine learning or any data specific task, you should partition the data correctly. For the model to generalize well, you split the training data into two parts, one designed for training and another one for validation. In this case, you will train the model on 80\\% of the training data and validate it on 20\\% of the remaining training data. This will also help to reduce overfitting since you will be validating the model on the data it would not have seen in training phase, which will help in boosting the test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "configured-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-little",
   "metadata": {},
   "source": [
    "For one last time let's check the shape of training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "polished-applicant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 28, 28, 1), (12000, 28, 28, 1), (48000, 10), (12000, 10))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-scout",
   "metadata": {},
   "source": [
    "## The Network\n",
    "The images are of size 28 x 28. You convert the image matrix to an array, rescale it between 0 and 1, reshape it so that it's of size 28 x 28 x 1, and feed this as an input to the network.\n",
    "\n",
    "You'll use three convolutional layers:\n",
    "\n",
    "The first layer will have 32-3 x 3 filters,\n",
    "The second layer will have 64-3 x 3 filters and\n",
    "The third layer will have 128-3 x 3 filters.\n",
    "In addition, there are three max-pooling layers each of size 2 x 2.\n",
    "\n",
    "### Model the Data\n",
    "First, let's import all the necessary modules required to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "enhanced-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-train",
   "metadata": {},
   "source": [
    "You will use a batch size of 64 using a higher batch size of 128 or 256 is also preferable it all depends on the memory. It contributes massively to determining the learning parameters and affects the prediction accuracy. You will train the network for 10 epochs (changed by hof@ - original training is 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "anonymous-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-bargain",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "In Keras, you can just stack up layers by adding the desired layer one by one. That's exactly what you'll do here: you'll first add a first convolutional layer with Conv2D(). Note that you use this function because you're working with images! Next, you add the Leaky ReLU activation function which helps the network learn non-linear decision boundaries. Since you have ten different classes, you'll need a non-linear decision boundary that could separate these ten classes which are not linearly separable.\n",
    "\n",
    "More specifically, you add Leaky ReLUs because they attempt to fix the problem of dying Rectified Linear Units (ReLUs). The ReLU activation function is used a lot in neural network architectures and more specifically in convolutional networks, where it has proven to be more effective than the widely used logistic sigmoid function. As of 2017, this activation function is the most popular one for deep neural networks. The ReLU function allows the activation to be thresholded at zero. However, during the training, ReLU units can \"die\". This can happen when a large gradient flows through a ReLU neuron: it can cause the weights to update in such a way that the neuron will never activate on any data point again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. Leaky ReLUs attempt to solve this: the function will not be zero but will instead have a small negative slope.\n",
    "\n",
    "Next, you'll add the max-pooling layer with MaxPooling2D() and so on. The last layer is a Dense layer that has a softmax activation function with 10 units, which is needed for this multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "defensive-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-jordan",
   "metadata": {},
   "source": [
    "## Compile the Model\n",
    "After the model is created, you compile it using the Adam optimizer, one of the most popular optimization algorithms. You can read more about this optimizer here. Additionally, you specify the loss type which is categorical cross entropy which is used for multi-class classification, you can also use binary cross-entropy as the loss function. Lastly, you specify the metrics as accuracy which you want to analyze while the model is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "amino-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-center",
   "metadata": {},
   "source": [
    "Let's visualize the layers that you created in the above step by using the summary function. This will show some parameters (weights and biases) in each layer and also the total parameters in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "involved-taiwan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 356,234\n",
      "Trainable params: 356,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-beatles",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "It's finally time to train the model with Keras' fit() function! The model trains for 20 epochs. The fit() function will return a history object; By storying the result of this function in fashion_train, you can use it later to plot the accuracy and loss function plots between training and validation which will help you to analyze your model's performance visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 223s 593ms/step - loss: 0.0240 - accuracy: 0.9925 - val_loss: 0.0300 - val_accuracy: 0.9907\n",
      "Epoch 2/5\n",
      " 68/375 [====>.........................] - ETA: 2:48 - loss: 0.0122 - accuracy: 0.9961"
     ]
    }
   ],
   "source": [
    "train = model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-freedom",
   "metadata": {},
   "source": [
    "Finally! You trained the model on fashion-MNIST for 20 epochs, and by observing the training accuracy and loss, you can say that the model did a good job since after 20 epochs the training accuracy is 99% and the training loss is quite low.\n",
    "\n",
    "However, it looks like the model is overfitting, as the validation loss is 0.4396 and the validation accuracy is 92%. Overfitting gives an intuition that the network has memorized the training data very well but is not guaranteed to work on unseen data, and that is why there is a difference in the training and validation accuracy.\n",
    "\n",
    "You probably need to handle this. In next sections, you'll learn how you can make your model perform much better by adding a Dropout layer into the network and keeping all the other layers unchanged.\n",
    "\n",
    "But first, let's evaluate the performance of your model on the test set before you come on to a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-thomas",
   "metadata": {},
   "source": [
    "## Model Evaluation on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "conservative-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04458894208073616\n",
      "Test accuracy: 0.9871000051498413\n"
     ]
    }
   ],
   "source": [
    "test_eval = model.evaluate(test_X, test_Y_one_hot, verbose=0)\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-hometown",
   "metadata": {},
   "source": [
    "The test accuracy looks impressive. It turns out that your classifier does better than the benchmark that was reported here, which is an SVM classifier with mean accuracy of 0.897. Also, the model does well compared to some of the deep learning models mentioned on the GitHub profile of the creators of fashion-MNIST dataset.\n",
    "\n",
    "However, you saw that the model looked like it was overfitting. Are these results really all that good?\n",
    "\n",
    "Let's put your model evaluation into perspective and plot the accuracy and loss plots between training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "durable-mechanics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = train.history['accuracy']\n",
    "val_accuracy = train.history['val_accuracy']\n",
    "loss = train.history['loss']\n",
    "val_loss = train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
